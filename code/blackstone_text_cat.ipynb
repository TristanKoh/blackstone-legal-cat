{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.5 64-bit ('blackstone': venv)",
   "display_name": "Python 3.7.5 64-bit ('blackstone': venv)",
   "metadata": {
    "interpreter": {
     "hash": "0660c2531e1297108d424e4980bfab9e203b56f1cdc22a94eebbcfd6c6bda3c2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Legal Text Classification with Blackstone\n",
    "\n",
    "This Jupyter notebook demonstrates the uses of Blackstone on Singaporean case law. By doing so, I hope to encourage others to get their hands dirty with basic programming and data science, especially law students that are interested in legal technology. I personally believe that one cannot simply talk about \"technology\" in the abstract when formulating legal rules that govern such technology.\n",
    "\n",
    "At the same time, I empathise with those who may be apprehensive of programming / coding, as I was 1.5 years ago. Hence, through this notebook, I aim to explain each step in the code as simply as possible, to demonstrate that one does not need to be particularly talented to self-learn programming."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## About Blackstone\n",
    "\n",
    "Blackstone is a Python package that uses Natural Language Processing (NLP) techniques to detect linguistic features in case law and classify the text into 5 categories.\n",
    "\n",
    "The five categories are:\n",
    "\n",
    "AXIOM - The text appears to postulate a well-established principle\n",
    "\n",
    "CONCLUSION - The text appears to make a finding, holding, determination or conclusion\n",
    "\n",
    "ISSUE - The text appears to discuss an issue or question\n",
    "\n",
    "LEGAL_TEST - The test appears to discuss a legal test\n",
    "\n",
    "UNCAT - The text does not fall into one of the four categories above\n",
    "\n",
    "To demonstrate the usage and performance of Blackstone, I used the seminal Singaporean tort case of Spandeck v DSTA Agency (2007) 4 SLR(R) 100.\n",
    "\n",
    "The rest of this jupyter notebook documents the text cleaning process and the prediction of the above legal categories."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Importing relevant packages and the data file\n",
    "\n",
    "Before we begin, certain packages need to be imported such that it would allow neater handling of the data. Packages are basically code written in Python that provide specific functionality that are not present in Python itself.\n",
    "\n",
    "The key packages that are used for this are:\n",
    "\n",
    "1. Pandas - Enables the structuring of data in a tabular format, with rows and columns (similar to an Excel spreadsheet).\n",
    "\n",
    "2. Blackstone - As mentioned, contains functionality for recognising legal text.\n",
    "\n",
    "3. Path - Auxilliary package that automates the creation of the relative file path to the text file that contains the raw text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\Users\\\\Tristan\\\\Desktop\\\\Projects\\\\blackstone\\\\blackstone-legal-cat\\\\code'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# To check working directory, affects the dataFilePath as defined below for the import of the spandeck file\n",
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# DatafilePath is a string that contains the relative file path to the data file\n",
    "dataFilePath = Path(\"..\", \"data\", \"spandeck.txt\")\n",
    "\n",
    "# Import the spandeck case as text\n",
    "spandeck = open(dataFilePath, \"r\", encoding= \"utf8\")\n",
    "\n",
    "# .readlines() returns a stream (ie. the text is not saved in memory), hence we save it into a string saved in memory\n",
    "text = spandeck.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy.load() loads the specific NLP model\n",
    "nlp = spacy.load(\"en_blackstone_proto\")"
   ]
  },
  {
   "source": [
    "### Function for text pre-processing\n",
    "\n",
    "This function replaces tabs and new lines and appends the sentences together to form a string."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def text_preprocessing(text):\n",
    "    \"\"\" Accepts a list of unprocessed strings, returns a list of strings without string and tab breaks and empty strings \"\"\"\n",
    "    \n",
    "    processed_text = []\n",
    "\n",
    "    for string in text:\n",
    "        string = string.replace(\"\\n\", \"\")\n",
    "        string = string.replace(\"\\t\", \" \")\n",
    "        processed_text.append(string)\n",
    "    \n",
    "    processed_text = [string for string in processed_text if string != \"\"]\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Run the function on the string\n",
    "text = text_preprocessing(text)"
   ]
  },
  {
   "source": [
    "## Function to split the text into individual strings instead of one entire string\n",
    "\n",
    "Since blackstone categorises at a sentence level, the function splits the text into individual sentences using blackstone's sentence boundary detector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legal_cats(sentences):\n",
    "    \"\"\"\n",
    "    Function to identify the highest scoring category prediction generated by the text categoriser. \n",
    "\n",
    "    Arguments: \n",
    "    a list of strings\n",
    "    \n",
    "    converts to spacy generator object, splits into sentences using spacy's sentence detector\n",
    "\n",
    "    returns a tuple of: \n",
    "    a list of the split sentences,\n",
    "    a list of the max cat and max score for each doc in tuples\n",
    "    \"\"\"\n",
    "    doc_sentences = []\n",
    "\n",
    "    docs = nlp.pipe(sentences, disable = [\"tagger\", \"ner\", \"textcat\"])\n",
    "\n",
    "    for doc in docs:\n",
    "        for sentence in doc.sents:\n",
    "            doc_sentences.append(sentence.text)\n",
    "    \n",
    "    docs = nlp.pipe(doc_sentences, disable = [\"tagger\", \"parser\", \"ner\"])\n",
    "    cats_list = []\n",
    "\n",
    "    for doc in docs:\n",
    "        cats = doc.cats\n",
    "        max_score = max(cats.values()) \n",
    "        max_cats = [k for k, v in cats.items() if v == max_score]\n",
    "        max_cat = max_cats[0]\n",
    "        cats_list.append((max_cat, max_score))\n",
    "\n",
    "    return doc_sentences, cats_list"
   ]
  },
  {
   "source": [
    "Parse the text through the sentence boundary detector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = legal_cats(text)"
   ]
  },
  {
   "source": [
    "Appending each sentence to a row of the dataframe\n",
    "\n",
    "Running the blackstone model on each row to classify the sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"sentence\" : cats[0], \"category\": [cat[0] for cat in cats[1]], \"score\": [cat[1] for cat in cats[1]]})\n",
    "\n",
    "df.head()\n",
    "\n",
    "df[\"category\"].unique()\n",
    "\n",
    "for sentence in df.loc[df[\"category\"] == \"LEGAL_TEST\", \"sentence\"][:30]:\n",
    "    print(sentence)\n",
    "    print(\"-\" * 40)"
   ]
  }
 ]
}