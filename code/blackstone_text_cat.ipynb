{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.5 64-bit",
   "display_name": "Python 3.7.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0660c2531e1297108d424e4980bfab9e203b56f1cdc22a94eebbcfd6c6bda3c2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Legal Text Classification with Blackstone\n",
    "\n",
    "### Author: Tristan Koh, NUS Law Year 2\n",
    "### GitHub: https://github.com/TristanKoh\n",
    "\n",
    "This Jupyter notebook demonstrates the uses of Blackstone on Singaporean case law. By doing so, I hope to encourage others to get their hands dirty with basic programming and data science, especially law students that are interested in legal technology. Even for students whose interest lies in the law of technology rather than technology of law, I personally believe that one cannot simply discuss \"technology\" in the abstract when formulating legal rules that govern such technology.\n",
    "\n",
    "At the same time, I empathise with those who may be apprehensive of programming / coding, as I was one and a half years ago. Hence, through this notebook, I aim to explain each step in the code as simply as possible, to demonstrate that one does not need to be particularly talented to self-learn programming.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## About Blackstone\n",
    "\n",
    "Blackstone is a Python package that uses Natural Language Processing (NLP) techniques to detect linguistic features in case law and classify the text into 5 categories.\n",
    "\n",
    "The five categories are:\n",
    "\n",
    "AXIOM - The text appears to postulate a well-established principle\n",
    "\n",
    "CONCLUSION - The text appears to make a finding, holding, determination or conclusion\n",
    "\n",
    "ISSUE - The text appears to discuss an issue or question\n",
    "\n",
    "LEGAL_TEST - The test appears to discuss a legal test\n",
    "\n",
    "UNCAT - The text does not fall into one of the four categories above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How Blackstone fits into the broader data science context\n",
    "\n",
    "As computers cannot understand text as humans do, NLP packages like Blackstone provide a set of utilities that allows us to create a mathematical model of the text, such that computers are able to process natural language. We call these models \"text representations\". The simplest of text representations (not used in Blackstone) is the Bag-of-Words representation. It represents the text as a count of words in the document.\n",
    "\n",
    "As you may imagine, such a representation loses significant semantic meaning, as it ignores word order and relative frequency of words in the text. This means that commonly used but less meaningful words like \"can\" and \"one\" have higher weightage in the model than more meaningful words like \"technology\" and \"programming\". \n",
    "\n",
    "Therefore, there are other, more complicated text representations that retain more semantic meaning in the text, such as as a Tf-IDF representation (which is essentially a weighted count of words) and word embeddings.\n",
    "\n",
    "Blackstone uses the latter model. For brevity, this article (https://machinelearningmastery.com/what-are-word-embeddings/) better explains how word embeddings work much better than I can, so I shall not go further into the details here.\n",
    "\n",
    "To demonstrate the usage and performance of Blackstone, I used the seminal Singaporean tort case of Spandeck v DSTA Agency (2007) 4 SLR(R) 100.\n",
    "\n",
    "The rest of this jupyter notebook documents the text cleaning process and the prediction of the above legal categories."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Importing relevant packages and the data file\n",
    "\n",
    "Before we begin, certain packages need to be imported such that it would allow neater and more efficient management of the data. Packages are basically code written in Python that provide specific functionality that are not present in Python itself.\n",
    "\n",
    "The key packages that are used for this notebook are:\n",
    "\n",
    "1. Pandas - Enables the structuring of data in a tabular format, with rows and columns (similar to an Excel spreadsheet).\n",
    "\n",
    "2. Blackstone - As mentioned, a NLP package.\n",
    "\n",
    "3. Path - Auxilliary package that creates the relative file path to the file that contains case that we are going to test Blackstone on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'c:\\\\Users\\\\Tristan\\\\Desktop\\\\Projects\\\\blackstone\\\\blackstone-legal-cat\\\\code'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# This code checks the location of the working directory, affects the definition of dataFilePath as defined below for the import of the spandeck file\n",
    "# I have left the code commented since it only needs to be used for checking the file path before starting the rest of the project\n",
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# DatafilePath is a string that contains the relative file path to the data file\n",
    "dataFilePath = Path(\"..\", \"data\", \"spandeck.txt\")\n",
    "\n",
    "# Import the spandeck case as text\n",
    "spandeck = open(dataFilePath, \"r\", encoding= \"utf8\")\n",
    "\n",
    "# .readlines() returns a stream (ie. the text is not saved in memory), hence we save it as a string called \"text\" which is saved in memory\n",
    "text = spandeck.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code loads the blackstone NLP model, and saves it into the object called NLP\n",
    "import blackstone\n",
    "import en_blackstone_proto\n",
    "nlp = en_blackstone_proto.load()"
   ]
  },
  {
   "source": [
    "## 2. Text pre-processing\n",
    "\n",
    "As the text as extracted directly from the pdf file is not \"clean\" (ie. contains formatting and other characters that do not carry any semantic meaning), we will first need to pre-process the text to remove these unessential characters.\n",
    "\n",
    "There are various packages that come with pre-written functions that can be used for general situations, but here the text only contains line and tab breaks, and hence I have decided to define my own function that removes such text formatting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What is a function?\n",
    "\n",
    "A function in programming is similar to mathematical functions; there is an input and an output, and a bunch of pre-defined steps are applied onto the input.\n",
    "\n",
    "Apart from pre-defined functions (such as the \"print\" function), we can also define our own functions. We do so for our convenience, because we can reuse the same lines of code defined within the function later on just by calling the function name."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Function for text pre-processing\n",
    "\n",
    "This function replaces tabs and new lines and appends the sentences together to form a single string."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def text_preprocessing(text):\n",
    "    \"\"\" Accepts a list of unprocessed strings, returns a list of strings without string and tab breaks and empty strings \"\"\"\n",
    "    \n",
    "    # This creates an empty list\n",
    "    processed_text = []\n",
    "\n",
    "    # This is a for loop; it iterates through the strings in the text, and performs some operations on each string. Hence the name for loop: \"For\" each string, apply X operations on the string.\n",
    "\n",
    "    # In this case, for each string, we replace new lines (\"\\n\") with an empty string, and replace tabs (\"\\t\") with a space.\n",
    "    for string in text:\n",
    "        string = string.replace(\"\\n\", \"\")\n",
    "        string = string.replace(\"\\t\", \" \")\n",
    "        processed_text.append(string)\n",
    "    \n",
    "    # This is a list comprehension; a more concise way of expressing a for loop.\n",
    "    # We iterate through each string in the processed text, and we only retain strings which are not empty strings (since empty strings are meaningless in this context)\n",
    "\n",
    "    processed_text = [string for string in processed_text if string != \"\"]\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Run the function on the string\n",
    "text = text_preprocessing(text)"
   ]
  },
  {
   "source": [
    "### Function to split the text into individual strings\n",
    "\n",
    "Since blackstone predicts at a sentence level (ie. we cannot use the entire case as one string as an input to blackstone), this function splits the text into individual strings using blackstone's sentence boundary detector.\n",
    "\n",
    "The sentence boundary detector is a function within blackstone that detects individual sentences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legal_cats(sentences):\n",
    "    \"\"\"\n",
    "    Function to identify the highest scoring category prediction generated by the text categoriser. \n",
    "\n",
    "    Arguments: \n",
    "    a list of strings\n",
    "    \n",
    "    converts to spacy generator object, splits into sentences using spacy's sentence detector\n",
    "\n",
    "    returns a tuple of: \n",
    "    a list of the split sentences,\n",
    "    a list of the max cat and max score for each doc in tuples\n",
    "    \"\"\"\n",
    "    doc_sentences = []\n",
    "\n",
    "    # This passes the input string through the nlp model, and converts it to doc object\n",
    "    # This doc object contains both the original text, and tags the sentences with certain attributes, such as the sentence boundary detector.\n",
    "    # A doc corresponds to a string.\n",
    "\n",
    "    docs = nlp.pipe(sentences, disable = [\"tagger\", \"ner\", \"textcat\"])\n",
    "\n",
    "    # We loop through each document in the documents, and loop again through each sentence in the document, and append the sentence to doc_sentences, an empty list\n",
    "    for doc in docs:\n",
    "        for sentence in doc.sents:\n",
    "            doc_sentences.append(sentence.text)\n",
    "    \n",
    "    # We can now categorise each sentence into one of the five abovementioned categories.\n",
    "\n",
    "    # We convert the newly detected sentences into a doc object again, as it contains the categoriser attribute that we can use to predict\n",
    "    \n",
    "    docs = nlp.pipe(doc_sentences, disable = [\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "    # We create a list to store the corresponding category and the score (ie the likelihood of the category that blackstone predicts the sentence to be)\n",
    "    # This index of the list corresponds to doc_sentences (ie. the first item in cats_list contains the predicted category and score for the first sentence in doc_sentences, the second item in cats_list contains the predicted category and score for the second sentence, so on and so forth)\n",
    "\n",
    "    cats_list = []\n",
    "\n",
    "    # We loop through the doc (sentence) in the documents, and return the highest probability category and its score for each sentence\n",
    "\n",
    "    # We have to select the highest scoring category because blackstone provides the probability of all five categories which the sentence can fall under.\n",
    "\n",
    "    # We are only concerned with blackstone's best prediction, and hence we only save the highest scoring category.\n",
    "    for doc in docs:\n",
    "        cats = doc.cats\n",
    "        max_score = max(cats.values()) \n",
    "        max_cats = [k for k, v in cats.items() if v == max_score]\n",
    "        max_cat = max_cats[0]\n",
    "        cats_list.append((max_cat, max_score))\n",
    "\n",
    "    return doc_sentences, cats_list"
   ]
  },
  {
   "source": [
    "## 3. Predicting on the processed text\n",
    "\n",
    "With the above function defined, we can now finally use blackstone to predict the categories of the text. This just involves calling the function with the cleaned text as the argument."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = legal_cats(text)"
   ]
  },
  {
   "source": [
    "## 4. Saving the predictions to a dataframe\n",
    "\n",
    "Appending each sentence to a row of the dataframe\n",
    "\n",
    "Running the blackstone model on each row to classify the sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tort – Negligence – Duty of care – Applicable test to determine existence of duty of care – Relationship between two-stage test and incremental approach – Application of two-stage test comprising first proximity and second policy considerations with threshold consideration of factual foreseeability – Incremental approach as methodological aid in applying specific criterion of two-stage test\n----------------------------------------\nTort – Negligence – Duty of care – Applicable test to determine existence of duty of care – Whether type of damage claimed should result in different test – Application of single (two-stage) test irrespective of type of damage claimed\n----------------------------------------\n(1)    A single test should determine the imposition of a duty of care in all claims arising out of negligence, irrespective of the type of the damages claimed.\n----------------------------------------\nThere was no justification for a general exclusionary rule against recovery of all economic losses, nor was there a need to adopt a different test for such cases.\n----------------------------------------\nThe single test eliminated the perception that there were, at once, two or more tests which were equally applicable: at [71] and [72].\n----------------------------------------\n(2)    The test to determine the imposition of a duty of care was a two-stage test comprising of, first, proximity and, second, policy considerations, which were together preceded by the threshold question of factual foreseeability.\n----------------------------------------\nThe first stage of proximity required sufficient legal proximity between the claimant and defendant for a duty of care to arise.\n----------------------------------------\nThe focus was on the closeness of the relationship between the parties, including physical, circumstantial and causal proximity, supported by the twin criteria of voluntary assumption of responsibility and reliance.\n----------------------------------------\nAlcock v Chief Constable of South Yorkshire Police [1992] 1 AC 310 (refd)\n----------------------------------------\nArenson v Casson Beckman Rutley & Co [1977] AC 405 (refd)\n----------------------------------------\nCaparo Industries Plc v Dickman [1989] QB 653 (refd)\n----------------------------------------\nCaparo Industries Plc v Dickman [1990] 2 AC 605 (refd)\n----------------------------------------\nCustoms and Excise Commissioners v Barclays Bank plc [2005] 1 WLR 2082 (not folld)\n----------------------------------------\nCustoms and Excise Commissioners v Barclays Bank plc [2007] 1 AC 181 (refd)\n----------------------------------------\nDonoghue v Stevenson [1932] AC 562 (refd)\n----------------------------------------\nHay or Bourhill v Young [1943] AC 92 (refd)\n----------------------------------------\nHo Soo Fong v Standard Chartered Bank [2007] 2 SLR(R) 181; [2007] 2 SLR 181 (refd)\n----------------------------------------\nMan B&W Diesel S E Asia Pte Ltd v PT Bumi International Tankers [2004] 2 SLR(R) 300; [2004] 2 SLR 300 (refd)\n----------------------------------------\nMohd bin Sapri v Soil-Build (Pte) Ltd [1996] 2 SLR(R) 223; [1996] 2 SLR 505 (refd)\n----------------------------------------\nPacific Associates Inc v Baxter [1990] 1 QB 993 (folld)\n----------------------------------------\nRSP Architects Planners & Engineers v Ocean Front Pte Ltd [1995] 3 SLR(R) 653; [1996] 1 SLR 113 (refd)\n----------------------------------------\nSmith v Eric S Bush [1990] 1 AC 831 (refd)\n----------------------------------------\nSunny Metal & Engineering Pte Ltd v Ng Khim Ming Eric [2007] 1 SLR(R) 853; [2007] 1 SLR 853 (refd)\n----------------------------------------\nSunrise Crane, The [2004] 4 SLR(R) 715; [2004] 4 SLR 715 (refd)\n----------------------------------------\nTV Media Pte Ltd v De Cruz Andrea Heidi [2004] 3 SLR(R) 543; [2004] 3 SLR 543 (refd)\n----------------------------------------\n[Editorial note: The decision from which this appeal arose is reported at [2007] 1 SLR(R) 720.]\n----------------------------------------\n23     In considering the duty of care issue, the trial judge applied both the two-stage test in Anns v Merton London Borough Council [1978] AC 728 (“Anns”) and the three-part test in Caparo Industries Plc v Dickman [1990] 2 AC 605 (“Caparo”) and reached the same result that there was no duty of care, an implicit judicial recognition of the state of confusion which belabours the local law in respect to the applicable test to be used to determine the existence of a duty of care for cases involving claims for pure economic loss (“cases of pure economic loss”) or, indeed, cases involving claims of personal injuries and physical damage (collectively “cases of physical damage”).\n----------------------------------------\nApplicable test for duty of care\n----------------------------------------\n26     Before us, the appellant submitted that the proper test to ascertain the existence of a duty of care for cases of pure economic loss should be a combination of the common elements from both the two-stage test in Anns and the three-part test in Caparo, citing the views contained in Andrew Phang, Saw Cheng Lim & Gary Chan, “Of Precedent, Theory and Practice – The Case for a Return to Anns” [2006] Sing JLS 1.\n----------------------------------------\nA broad two-stage approach or any other approach is only a framework, a more or less methodical way of tackling a problem.\n----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"sentence\" : cats[0], \"category\": [cat[0] for cat in cats[1]], \"score\": [cat[1] for cat in cats[1]]})\n",
    "\n",
    "df.head()\n",
    "\n",
    "df[\"category\"].unique()\n",
    "\n",
    "for sentence in df.loc[df[\"category\"] == \"LEGAL_TEST\", \"sentence\"][:30]:\n",
    "    print(sentence)\n",
    "    print(\"-\" * 40)"
   ]
  }
 ]
}